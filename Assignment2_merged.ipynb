{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, copy\n",
    "from collections import defaultdict\n",
    "import numpy, re, math\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from string import punctuation\n",
    "from scipy import stats\n",
    "import pandas\n",
    "from collections import defaultdict\n",
    "from sklearn import linear_model, datasets\n",
    "from collections import Counter\n",
    "import random\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pandas.read_csv(\"C:\\Users\\Induja\\Documents\\CSE 258\\Assignment 2\\h1b_kaggle.csv\")\n",
    "data = data.set_index('Unnamed: 0')\n",
    "# Removing rows with NA in case_status\n",
    "data = data[pandas.notnull(data['CASE_STATUS'])]\n",
    "data = data.drop(data[data.CASE_STATUS == 'CERTIFIED-WITHDRAWN'].index)\n",
    "data = data.drop(data[data.CASE_STATUS == 'INVALIDATED'].index)\n",
    "data = data.drop(data[data.CASE_STATUS == 'WITHDRAWN'].index)\n",
    "data = data.drop(data[data.CASE_STATUS == 'PENDING QUALITY AND COMPLIANCE REVIEW - UNASSIGNED'].index)\n",
    "\n",
    "# To keep rows which have employer names that are among the ~1200 most frequent\n",
    "#threshold = 0.0001\n",
    "#for col in df:\n",
    "#col = 'EMPLOYER_NAME'\n",
    "#counts = data[col].value_counts(normalize=True)\n",
    "#data = data.loc[data[col].isin(counts[counts > threshold].index), :]\n",
    "data = data[data.CASE_STATUS != 'NA']\n",
    "data = data[data.FULL_TIME_POSITION != 'NA']\n",
    "\n",
    "# Case_status = 1 for certified/certified-withdrawn, 0 for denied\n",
    "data['CASE_STATUS'] = data['CASE_STATUS'].map({'CERTIFIED': 1, 'DENIED': 0, 'REJECTED': 2})\n",
    "data['CASE_STATUS'].replace(2,0, inplace=True)\n",
    "data['FULL_TIME_POSITION'] = data['FULL_TIME_POSITION'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "# Separating states and adding that as another column\n",
    "states = []\n",
    "for row in data.iterrows():\n",
    "    st = row[1]['WORKSITE'].split(',')[-1][1:]\n",
    "    states.append(st)\n",
    "s = pandas.Series(states)\n",
    "data['STATES'] = s.values\n",
    "\n",
    "data = data[data.STATES != 'NA']\n",
    "data = data[data.FULL_TIME_POSITION.notnull()]\n",
    "data = data[data.PREVAILING_WAGE.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg = data[data.CASE_STATUS == 0]\n",
    "pos = data[data.CASE_STATUS == 1]\n",
    "pos = pos.sample(frac=0.04).reset_index(drop=True)  # Random sampling from +ve data, to keep 0.04 times the original\n",
    "frames = [pos, neg]\n",
    "result = pandas.concat(frames)\n",
    "result = result.sample(frac=1).reset_index(drop=True)  # Shuffling result\n",
    "result = result[result.PREVAILING_WAGE.notnull()]\n",
    "# Making buckets for wages, boundaries given in bins=[]\n",
    "result['wage_bucket'] = pandas.cut(result['PREVAILING_WAGE'], bins=[0, 50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000, 150000, 200000, 1000000000], labels=False)\n",
    "result = result.dropna(subset=['wage_bucket'])\n",
    "result.wage_bucket = result.wage_bucket.astype(int)\n",
    "result.YEAR = result.YEAR.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_map = {\n",
    "    'ALABAMA': 1,\n",
    "    'ALASKA': 2,\n",
    "    'ARIZONA': 3,\n",
    "    'ARKANSAS': 4,\n",
    "    'CALIFORNIA': 5,\n",
    "    'COLORADO': 6,\n",
    "    'CONNECTICUT': 7,\n",
    "    'DELAWARE': 8,\n",
    "    'DISTRICT OF COLUMBIA': 9,\n",
    "    'FLORIDA': 10,\n",
    "    'GEORGIA': 11,\n",
    "    'HAWAII': 12,\n",
    "    'IDAHO': 13,\n",
    "    'ILLINOIS': 14,\n",
    "    'INDIANA': 15,\n",
    "    'IOWA': 16,\n",
    "    'KANSAS': 17,\n",
    "    'KENTUCKY': 18,\n",
    "    'LOUISIANA': 19,\n",
    "    'MAINE': 20,\n",
    "    'MARYLAND': 21,\n",
    "    'MASSACHUSETTS': 22,\n",
    "    'MICHIGAN': 23,\n",
    "    'MINNESOTA': 24,\n",
    "    'MISSISSIPPI': 25,\n",
    "    'MISSOURI': 26,\n",
    "    'MONTANA': 27,\n",
    "    'NEBRASKA': 28,\n",
    "    'NEVADA': 29,\n",
    "    'NEW HAMPSHIRE': 30,\n",
    "    'NEW JERSEY': 31,\n",
    "    'NEW MEXICO': 32,\n",
    "    'NEW YORK': 33,\n",
    "    'NORTH CAROLINA': 34,\n",
    "    'NORTH DAKOTA': 35,\n",
    "    'OHIO': 36,\n",
    "    'OKLAHOMA': 37,\n",
    "    'OREGON': 38,\n",
    "    'PENNSYLVANIA': 39,\n",
    "    'PUERTO RICO': 40,\n",
    "    'RHODE ISLAND': 41,\n",
    "    'SOUTH CAROLINA': 42,\n",
    "    'SOUTH DAKOTA': 43,\n",
    "    'TENNESSEE': 44,\n",
    "    'TEXAS': 45,\n",
    "    'UTAH': 46,\n",
    "    'VERMONT': 47,\n",
    "    'VIRGINIA': 48,\n",
    "    'WASHINGTON': 49,\n",
    "    'WEST VIRGINIA': 50,\n",
    "    'WISCONSIN': 51,\n",
    "    'WYOMING': 52}\n",
    "\n",
    "def classifierStats(pred, y):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(pred)):\n",
    "        if y[i] == 1:\n",
    "            if pred[i] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if pred[i] == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "    FPR = 1.0*fp/(tn+fp)\n",
    "    FNR = 1.0*fn/(tp+fn)\n",
    "    print(\"Accuracy = \" + str(1.0*(tp + tn)/(tn+fp+fn+tp)))\n",
    "    print(\"FPR = \" + str(FPR))\n",
    "    print(\"FNR = \" + str(FNR))\n",
    "    print(\"BER = \" + str(0.5*(FPR+FNR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wage_buck_num = len(result.wage_bucket.unique())\n",
    "employer_freq = result.groupby('EMPLOYER_NAME').count().CASE_STATUS\n",
    "employer_accept = pos.groupby('EMPLOYER_NAME').count().CASE_STATUS \n",
    "soc_freq = result.groupby('SOC_NAME').count().CASE_STATUS\n",
    "soc_accept = pos.groupby('SOC_NAME').count().CASE_STATUS     \n",
    "def feature(datum):\n",
    "    feat = [0]*(1+6+52+wage_buck_num+2+2+1)\n",
    "    feat[0] = 1\n",
    "    feat[datum['YEAR'] - 2010] = 1\n",
    "    feat[state_map[datum['STATES']]+6] = 1\n",
    "    #feat[employers_id[datum['EMPLOYER_NAME']]+len(state_map)+6] = 1\n",
    "    feat[datum['wage_bucket']+6+52] = 1\n",
    "    feat[-1] = datum['FULL_TIME_POSITION']\n",
    "    e = datum['EMPLOYER_NAME']\n",
    "    if e in employer_freq:\n",
    "        feat[-2] = employer_freq[e]\n",
    "        if e in employer_accept:\n",
    "            feat[-3] = employer_accept[e] * 1.0 / feat[-2]\n",
    "    soc = datum['SOC_NAME']\n",
    "    if soc in soc_freq:\n",
    "        feat[-4] = soc_freq[soc]\n",
    "        if soc in soc_accept:\n",
    "            feat[-5] = soc_accept[soc] * 1.0 / feat[-4]\n",
    "    return feat\n",
    "\n",
    "X_train = [feature(d[1]) for d in result.loc[:len(result)/2,:].iterrows()]\n",
    "y_train = [d[1]['CASE_STATUS'] for d in result.loc[:len(result)/2,:].iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test =  [feature(d[1]) for d in result.loc[len(result)/2:,:].iterrows()]\n",
    "y_test =  [d[1]['CASE_STATUS'] for d in result.loc[len(result)/2:,:].iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.855586518851\n",
      "FPR = 0.214814656736\n",
      "FNR = 0.0811950251998\n",
      "BER = 0.148004840968\n"
     ]
    }
   ],
   "source": [
    "logreg = linear_model.LogisticRegression(C=10)\n",
    "logreg.fit(X_train, y_train)\n",
    "pred = logreg.predict(X_test)\n",
    "classifierStats(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.850750186789\n",
      "FPR = 0.199278687125\n",
      "FNR = 0.104325163367\n",
      "BER = 0.151801925246\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 500, max_depth =40, max_features=10)\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_test)\n",
    "classifierStats(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=10, kernel='rbf')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n",
    "classifierStats(test_predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.526877486319\n",
      "FPR = 1.0\n",
      "FNR = 0.0\n",
      "BER = 0.5\n"
     ]
    }
   ],
   "source": [
    "#Baseline 1 : ZeroR\n",
    "y_consolidated = y_train + y_test \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def Most_Common(y_consolidated):\n",
    "    data = Counter(y_consolidated)\n",
    "    return data.most_common(1)[0][0]\n",
    "\n",
    "pred_baselise1 = [Most_Common(y_consolidated)] * len(y_test)\n",
    "\n",
    "#Accuracy & FNR & FPR\n",
    "classifierStats(pred_baselise1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.500676480685\n",
      "FPR = 0.525726114514\n",
      "FNR = 0.475614663779\n",
      "BER = 0.500670389147\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#Baseline 2 : Biased Coin Toss\n",
    "sum = numpy.sum(y_consolidated)\n",
    "sum = sum*1.0/len(y_consolidated)\n",
    "#Accuracy\n",
    "pred_baselise2 = [ (random.random() < sum) for x in range(len(y_test)) ]\n",
    "#FNR & FPR\n",
    "classifierStats(pred_baselise2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
